{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generator_gpt2_libraries.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YFCOifqip0xk"},"source":["# General Libraries & functions for Generator Files: GPT2...\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RP2S6ANqzO2E"},"source":["## 0. Create progress reporting mechanism"]},{"cell_type":"code","metadata":{"id":"NOfVzQL1pM_8"},"source":["from IPython.display import clear_output\n","\n","progress_output = \"generator_gpt2_libraries.ipynb:\"\n","\n","def update_report(progress_output, update_text):\n","  progress_output = progress_output + \"\\n\" + update_text\n","  clear_output()\n","  print(progress_output)\n","  return progress_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JGv_hMXuqCMv"},"source":["## 1. Importing and including libraries"]},{"cell_type":"code","metadata":{"id":"MMZutYylp8ns"},"source":["print(\"Installing required libraries...\")\n","\n","!pip install tokenizers\n","!pip install transformers \n","\n","!pip install fastai==2.0.15\n","!pip install fastai2==0.0.30\n","!pip install fastcore==1.0.16\n","\n","!pip install -Uqq fastbook\n","\n","#Update progress\n","progress_output = update_report(progress_output,\"Libraries Installed.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLvHmuwEp9l0"},"source":["print(\"Importing required libraries...\")\n","import pandas as pd\n","\n","from fastai.text.all import *\n","\n","import fastbook\n","from fastbook import *\n","fastbook.setup_book()\n","\n","# Import GPT2 tokenizer\n","from transformers import GPT2TokenizerFast # for documentation: https://huggingface.co/transformers/_modules/transformers/tokenization_gpt2.html\n","\n","#Update progress\n","progress_output = update_report(progress_output,\"Libraries Imported.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9xn1b1uHqHhr"},"source":["## 2. Loading functions"]},{"cell_type":"code","metadata":{"id":"8_wcUTlOpt0s"},"source":["# To process this data to train a model, we need to build a Transform that will be applied lazily.\n","\n","class TransformersTokenizer(Transform):\n","    def __init__(self, tokenizer): self.tokenizer = tokenizer\n","    def encodes(self, x): \n","        toks = self.tokenizer.tokenize(x)\n","        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n","    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m0EfwfHWpuoX"},"source":["# We use callbacks in case we want to alter the behavior of the training loop \n","class DropOutput(Callback):\n","    def after_pred(self): self.learn.pred = self.pred[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9eEr2WRmqJUF"},"source":["# Generate Output\n","def gen_story(my_model, my_tokenizer, seed, max_len, \n","              TEMP = 0.6,\n","              TOP_K = 40,\n","              TOP_P = 0.85):\n","\n","  # take input\n","  prompt_ids = my_tokenizer.encode(seed)\n","  inp = tensor(prompt_ids)[None]#.cuda() # un-do .cuda() if no GPU available\n"," \n","  # generate output\n","  sample_outputs = my_model.generate(\n","                              inp,\n","                              do_sample = True, \n","                              max_length = max_len,     \n","                              temperature = TEMP,\n","                              top_k = TOP_K, \n","                              top_p = TOP_P, \n","                              num_return_sequences = 1\n","                              )\n","\n","  # Temperature is used to control the randomness of predictions by scaling the logits before applying softmax \n","  # (small (0.2): model is more confident but also more conservative, large( 1.0): more diversity but also more mistakes)\n","\n","  return my_tokenizer.decode(sample_outputs[0], skip_special_tokens = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5fEyG4fkwpEo"},"source":["model_path = data_path + \"models/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LW0aXYnbxDyG"},"source":["#Update progress\n","progress_output = update_report(progress_output,\"Functions Loaded and available for use:\\n    gen_story(my_model, my_tokenizer, seed, max_len)\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cP2CRr-60htG"},"source":["## 0. Clean up progress reporting mechanism"]},{"cell_type":"code","metadata":{"id":"X2-yOU1m0pUo"},"source":["gpt2_libraries_progress = progress_output\n","del(progress_output)\n","del(update_report)"],"execution_count":null,"outputs":[]}]}