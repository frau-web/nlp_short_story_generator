{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generator_gpt2S_tuned_on_tandc_unfrozen.ipynb","provenance":[{"file_id":"https://github.com/frau-web/nlp_short_story_generator/blob/main/generation/generator_gpt2S_tuned_on_tandc_unfrozen.ipynb","timestamp":1620562053129}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YFCOifqip0xk"},"source":["# Generator File: GPT2-Small Finetuned incl. unfreezing\n"]},{"cell_type":"markdown","metadata":{"id":"obIk5rLgBtK9"},"source":["This notebook loads the trained and saved model weights incl. functions and generates text output.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JGv_hMXuqCMv"},"source":["## 1. Importing libraries and functions"]},{"cell_type":"markdown","metadata":{"id":"HZzz62nKBupH"},"source":["We import the necessary libraries and functions incl. their classes:\n","\n","Imported libraries are:\n","\n","    Huggingface transformers & tokenizers\n","    Fastai\n","    Fastbook\n","    Pandas\n","\n","We trained our model by fine-tuning the pre-trained GPT2 model from the Huggingface Transformers Library with a corpus of children stories and through slowly training deeper layers of the network . The corpus was tokenized using the huggingface GPT2 tokenizer. We used the same tokenizer than the one used to train the original GPT2 model to ensure that the same splitting method is used for the new corpus as during pretraining. The GPT-2 tokenizer is based on Byte-Pair-Encodings (on byte-level).\n","\n","We also import a pre-defined function for text generation with the finetuned model weights.\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"j9EWVKO6tUbS"},"source":["try:\n","    print(gpt2_libraries_progress)\n","except NameError:\n","    %run ./generation/generator_gpt2_libraries.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Na340M7NqFId"},"source":["## 2. Importing model"]},{"cell_type":"markdown","metadata":{"id":"XKyEiC-4BzFF"},"source":["We import the finetuned model incl. its weights; for fine-tuning we used the pre-trained GPT2-Small model and trained it on a small dataset with texts for small children from the Gutenberg library by slowly unfreezing the top layers of the pre-trained GPT2-Small model."]},{"cell_type":"code","metadata":{"id":"rDzrNG7tupGJ"},"source":["print(\"Loading model...\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4ZDsDG8tB11"},"source":["# Split a GPT2 model in 4 groups for differential learning rates (Code from Finetuning English GPT2 to any language)\n","\n","def splitter(model):\n","    \"Split a GPT2 `model` in 3 groups for differential learning rates.\"\n","    \n","    # First layers group : decoder blocks from 0 to 3\n","    modules = []\n","    for i in range(4): modules.append(model.transformer.h[i])\n","    groups = [nn.Sequential(*modules)]\n","\n","    # Second layers group : decoder blocks from 4 to 7\n","    modules = []\n","    for i in range(4,8,1): modules.append(model.transformer.h[i])\n","    groups = L(groups + [nn.Sequential(*modules)])\n","\n","    # Third layers group : decoder blocks from 8 to 11\n","    modules = []\n","    for i in range(8,12,1): modules.append(model.transformer.h[i])\n","    groups = L(groups + [nn.Sequential(*modules)])\n","    \n","    # Fourth layers group : embeddings matrices wte and wpe + LayerNorm at the model output\n","    groups = L(groups + [nn.Sequential(model.transformer.wte,model.transformer.wpe,model.transformer.ln_f)])\n","    \n","    return groups.map(params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EWqgW7XsGyz"},"source":["# Load model\n","gpt2S_tuned_on_tandc_unfrozen = load_learner(model_path + \"gpt2S_tuned_on_tandc_unfrozen.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tS78RmjzbOeQ"},"source":["# Define tokenizer\n","gpt2s_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', add_prefix_space=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVfp-qYcWQAa"},"source":["##3. Loading functions"]},{"cell_type":"markdown","metadata":{"id":"iVVf2g9XB2-p"},"source":["We import and run the text generation function. It is based on the model.generate() function and random sampling technique, with the following hyperparameters:\n","- TEMP = 0.6: Temperature is used to control the randomness of predictions by scaling the logits before applying softmax. With a small temperate (e.g. 0.2), the model is more confident but also more conservative, with a large temperature (e.g. 1.0), the model generates more diversity but also makes more mistakes\n","- TOP_K = 40: for random text generation, the Top-K words in terms of probability of occurrence are selected and considered for the conditional probability distribution. This helps in avoiding repeating text\n","- TOP_P = 0.85: Top-P sampling (also: nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely words, we choose the smallest set of words whose total probability is larger than p. \n","Using both top-k and top-p reduces the chances of getting weird (rare) words while allowing for dynamic word selection. "]},{"cell_type":"code","metadata":{"id":"dwQqACPmW44J"},"source":["def gen_story_gpt2s_tunedonTC_unfrozen(seed, max_len):\n","  return gen_story(my_model = gpt2S_tuned_on_tandc_unfrozen, my_tokenizer = gpt2s_tokenizer, seed=seed, max_len=max_len) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7f334Vu4Hpp"},"source":["clear_output()\n","print(\"Function and Model now available for use:\")\n","print(\"    gen_story_gpt2s_tunedonTC_unfrozen(seed, max_len)\")\n","print(\"    *Based on GPT2-Small  | Tuned on input_stories_toddlerpluschildren.txt | Unfrozen\")"],"execution_count":null,"outputs":[]}]}