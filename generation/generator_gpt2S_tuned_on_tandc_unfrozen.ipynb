{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generator_gpt2S_tuned_on_tandc_unfrozen.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YFCOifqip0xk"},"source":["# Generator File: GPT2-Small Finetuned incl. unfreezing\n"]},{"cell_type":"markdown","metadata":{"id":"JGv_hMXuqCMv"},"source":["## 1. Importing libraries and functions"]},{"cell_type":"code","metadata":{"id":"j9EWVKO6tUbS"},"source":["try:\n","    print(gpt2_libraries_progress)\n","except NameError:\n","    %run ./generation/generator_gpt2_libraries.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Na340M7NqFId"},"source":["## 2. Importing model"]},{"cell_type":"code","metadata":{"id":"rDzrNG7tupGJ"},"source":["print(\"Loading model...\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4ZDsDG8tB11"},"source":["# Split a GPT2 model in 4 groups for differential learning rates (Code from Finetuning English GPT2 to any language)\n","\n","def splitter(model):\n","    \"Split a GPT2 `model` in 3 groups for differential learning rates.\"\n","    \n","    # First layers group : decoder blocks from 0 to 3\n","    modules = []\n","    for i in range(4): modules.append(model.transformer.h[i])\n","    groups = [nn.Sequential(*modules)]\n","\n","    # Second layers group : decoder blocks from 4 to 7\n","    modules = []\n","    for i in range(4,8,1): modules.append(model.transformer.h[i])\n","    groups = L(groups + [nn.Sequential(*modules)])\n","\n","    # Third layers group : decoder blocks from 8 to 11\n","    modules = []\n","    for i in range(8,12,1): modules.append(model.transformer.h[i])\n","    groups = L(groups + [nn.Sequential(*modules)])\n","    \n","    # Fourth layers group : embeddings matrices wte and wpe + LayerNorm at the model output\n","    groups = L(groups + [nn.Sequential(model.transformer.wte,model.transformer.wpe,model.transformer.ln_f)])\n","    \n","    return groups.map(params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EWqgW7XsGyz"},"source":["# Load model\n","gpt2S_tuned_on_tandc_unfrozen = load_learner(model_path + \"gpt2S_tuned_on_tandc_unfrozen.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tS78RmjzbOeQ"},"source":["# Define tokenizer\n","gpt2s_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2', add_prefix_space=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVfp-qYcWQAa"},"source":["##3. Loading functions"]},{"cell_type":"code","metadata":{"id":"dwQqACPmW44J"},"source":["def gen_story_gpt2s_tunedonTC_unfrozen(seed, max_len):\n","  return gen_story(my_model = gpt2S_tuned_on_tandc_unfrozen, my_tokenizer = gpt2s_tokenizer, seed=seed, max_len=max_len) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7f334Vu4Hpp"},"source":["clear_output()\n","print(\"Function and Model now available for use:\")\n","print(\"    gen_story_gpt2s_tunedonTC_unfrozen(seed, max_len)\")\n","print(\"    *Based on GPT2-Small  | Tuned on input_stories_toddlerpluschildren.txt | Unfrozen\")"],"execution_count":null,"outputs":[]}]}