{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generator_gpt2M_tuned_on_toddler.ipynb","provenance":[{"file_id":"https://github.com/frau-web/nlp_short_story_generator/blob/main/generation/generator_gpt2M_tuned_on_toddler.ipynb","timestamp":1620561782241}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YFCOifqip0xk"},"source":["# Generator File: GPT2-Medium Finetuned"]},{"cell_type":"markdown","metadata":{"id":"3ihdLSCw__qf"},"source":["This notebook loads the trained and saved model weights incl. functions and generates text output. "]},{"cell_type":"markdown","metadata":{"id":"JGv_hMXuqCMv"},"source":["## 1. Importing libraries and functions"]},{"cell_type":"markdown","metadata":{"id":"M3WKMX50AAyz"},"source":["We import the necessary libraries and functions incl. their classes: \n","\n","Imported libraries are: \n","- Huggingface transformers & tokenizers\n","- Fastai\n","- Fastbook\n","- Pandas\n","\n","\n","We trained our model by fine-tuning the pre-trained GPT2 model from the Huggingface Transformers Library with a corpus of children stories. The corpus was tokenized using the huggingface GPT2 tokenizer. We used the same tokenizer than the one used to train the original GPT2 model to ensure that the same splitting method is used for the new corpus as during pretraining. The GPT-2 tokenizer is based on Byte-Pair-Encodings (on byte-level). \n","\n","We also import a pre-defined function for text generation with the finetuned model weights.          "]},{"cell_type":"code","metadata":{"id":"TFDH8XPeAPL4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620561914157,"user_tz":-120,"elapsed":2497,"user":{"displayName":"Martina Weberruss","photoUrl":"","userId":"10904308501421555335"}},"outputId":"2065349a-3197-47f5-c424-a088eb620489"},"source":["try:\n","    print(gpt2_libraries_progress)\n","except NameError:\n","    %run ./generation/generator_gpt2_libraries.ipynb"],"execution_count":1,"outputs":[{"output_type":"stream","text":["ERROR:root:File `'./generation/generator_gpt2_libraries.ipynb.py'` not found.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Na340M7NqFId"},"source":["## 2. Importing model"]},{"cell_type":"markdown","metadata":{"id":"S5-dT1lvAJ8j"},"source":["We import the finetuned model incl. its weights; for fine-tuning we used the pre-trained GPT2-Medium model and trained it on a small dataset with texts for small children from the Gutenberg library. \n","\n"]},{"cell_type":"code","metadata":{"id":"bGRUrrWButmw"},"source":["print(\"Loading model...\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tvrvbxEMas3a"},"source":["# Define tokenizer\n","gpt2m_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2-medium', add_prefix_space=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HbZOHpoCpyq7"},"source":["# Load model\n","gpt2M_tuned_on_toddler = load_learner(model_path + \"gpt2M_tuned_on_toddler.pkl\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVfp-qYcWQAa"},"source":["## 3. Loading functions"]},{"cell_type":"markdown","metadata":{"id":"nUr-GVkfAS3j"},"source":["We import and run the text generation function. It is based on the model.generate() function and random sampling technique, with the following hyperparameters:\n","- TEMP = 0.6: Temperature is used to control the randomness of predictions by scaling the logits before applying softmax. With a small temperate (e.g. 0.2), the model is more confident but also more conservative, with a large temperature (e.g. 1.0), the model generates more diversity but also makes more mistakes\n","- TOP_K = 40: for random text generation, the Top-K words in terms of probability of occurrence are selected and considered for the conditional probability distribution. This helps in avoiding repeating text\n","- TOP_P = 0.85: Top-P sampling (also: nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely words, we choose the smallest set of words whose total probability is larger than p. \n","Using both top-k and top-p reduces the chances of getting weird (rare) words while allowing for dynamic word selection. "]},{"cell_type":"code","metadata":{"id":"dwQqACPmW44J"},"source":["def gen_story_gpt2m_tunedonT(seed, max_len):\n","  return gen_story(my_model = gpt2M_tuned_on_toddler, my_tokenizer = gpt2m_tokenizer, seed=seed, max_len=max_len) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7f334Vu4Hpp"},"source":["clear_output()\n","print(\"Function and Model now available for use:\")\n","print(\"    gen_story_gpt2m_tunedonT(seed, max_len)\")\n","print(\"    *Based on GPT2-Medium | Tuned on input_stories_toddler.txt\")"],"execution_count":null,"outputs":[]}]}