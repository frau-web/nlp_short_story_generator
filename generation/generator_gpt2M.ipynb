{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generator_gpt2M.ipynb","provenance":[{"file_id":"https://github.com/frau-web/nlp_short_story_generator/blob/main/generation/generator_gpt2M.ipynb","timestamp":1620562316281}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YFCOifqip0xk"},"source":["# Generator File: GPT2-Medium"]},{"cell_type":"markdown","metadata":{"id":"6qtIgJcLCEHH"},"source":["This notebook loads the trained and saved model weights incl. functions and generates text output."]},{"cell_type":"markdown","metadata":{"id":"JGv_hMXuqCMv"},"source":["## 1. Importing libraries and functions"]},{"cell_type":"markdown","metadata":{"id":"-ufdhg3_CF-h"},"source":["We import the necessary libraries and functions incl. their classes:\n","\n","Imported libraries are:\n","\n","- Huggingface transformers & tokenizers\n","- Fastai\n","- Fastbook\n","- Pandas"]},{"cell_type":"code","metadata":{"id":"0jNg-wTt2-zV"},"source":["try:\n","    print(gpt2_libraries_progress)\n","except NameError:\n","    %run ./generation/generator_gpt2_libraries.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Na340M7NqFId"},"source":["## 2. Importing model"]},{"cell_type":"markdown","metadata":{"id":"QvfvzwiOCQoi"},"source":["We load the pre-trained GPT2-medium model from `huggingface`."]},{"cell_type":"code","metadata":{"id":"vqlxbW3cuumM"},"source":["print(\"Loading model...\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KHhdXASkYD_u"},"source":["# Define tokenizer\n","gpt2m_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2-medium', add_prefix_space=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SvNyBv6yp_T3"},"source":["# Import GPT2 pretrained model\n","from transformers import GPT2LMHeadModel\n","gpt2m = GPT2LMHeadModel.from_pretrained('gpt2-medium')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVfp-qYcWQAa"},"source":["##3. Loading functions"]},{"cell_type":"markdown","metadata":{"id":"6OfcPzbhCnZo"},"source":["We import and run the text generation function. It is based on the model.generate() function and random sampling technique, with the following hyperparameters:\n","- TEMP = 0.6: Temperature is used to control the randomness of predictions by scaling the logits before applying softmax. With a small temperate (e.g. 0.2), the model is more confident but also more conservative, with a large temperature (e.g. 1.0), the model generates more diversity but also makes more mistakes\n","- TOP_K = 40: for random text generation, the Top-K words in terms of probability of occurrence are selected and considered for the conditional probability distribution. This helps in avoiding repeating text\n","- TOP_P = 0.85: Top-P sampling (also: nucleus sampling) is similar to Top-K, but instead of choosing the top k most likely words, we choose the smallest set of words whose total probability is larger than p. \n","Using both top-k and top-p reduces the chances of getting weird (rare) words while allowing for dynamic word selection. "]},{"cell_type":"code","metadata":{"id":"dwQqACPmW44J"},"source":["def gen_story_gpt2m(seed, max_len):\n","  return gen_story(my_model = gpt2m, my_tokenizer = gpt2m_tokenizer, seed=seed, max_len=max_len) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7f334Vu4Hpp"},"source":["clear_output()\n","print(\"Function and Model now available for use:\")\n","print(\"    gen_story_gpt2m(seed, max_len)\")\n","print(\"    *Based on GPT2-Medium | Untuned\")"],"execution_count":null,"outputs":[]}]}