{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_GPT2s_finetuned using simple_gpt2",
      "provenance": [],
      "collapsed_sections": [
        "N8KXuKWzQSsN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yssyZCa-2mrv"
      },
      "source": [
        "# Finetuning GPT2 using simple library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK4EquSV2qeG"
      },
      "source": [
        "*Based on the notebook of [Max Woolf](https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0PC4q6z3NLx"
      },
      "source": [
        "In the following, we will train a pre-trained GPT2 model with a corpus of childrenbooks using  `gpt-2-simple` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAewhMSN3biS"
      },
      "source": [
        "## 1. Getting ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkHv7rtd9yFT"
      },
      "source": [
        "### 1.1. Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "575d5141-c596-4826-ab82-3db1402fc0ce"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "### 1.2. GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO3pq5oO93Sq"
      },
      "source": [
        "(Text from Max Woolf): Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f049e6f2-cbda-4c41-96ee-7be8a2b8c01f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  8 08:31:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD_6Wgjx97Ze"
      },
      "source": [
        "### 1.3. Downloading GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS"
      },
      "source": [
        "For fine-tuning and retraining, we will need to download the GPT-2 model first.\n",
        "\n",
        "We will use  the smallest version of the 4 available versions with `124M` parameters; it takes ca. 500 MB on disk (vs. the medium model with `335M` parameters and 1.5GB on disk). The large model (with `774M` parameters) as well as the x-large model (with `1558M` parameters) cannot be finetuned with Colab. \n",
        "\n",
        "The larger the model, the more knowledge it has but the longer it also takes to train and generate text as well as needing more space on disk. \n",
        "\n",
        "The next cell downloads the model from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4794a3b5-03b8-4d43-9f04-af9736e961fc"
      },
      "source": [
        "# Download small gpt2 model\n",
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 243Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 2.41Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 661Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:16, 6.55Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 318Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 3.64Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 2.79Mit/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN"
      },
      "source": [
        "### 1.4. Mounting Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu"
      },
      "source": [
        "Mount your Google Drive in the VM and upload the training text file there. We train our text on a handselected library of childrenbooks from the Gutenberg Library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0440d2-e358-49c7-e1f8-3e8931059475"
      },
      "source": [
        "# Mount Google Drive\n",
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "# Give file name of the training text (file available at: \"...data/stories/input_stories_toddlerpluschildren.txt\")\n",
        "file_name = \"input_stories_toddlerpluschildren.txt\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoyXII8F-ALp"
      },
      "source": [
        "## 2. Finetuning GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "(Text from Max Woolf): The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f24fe3-b11c-4beb-8884-729de11e2d1f"
      },
      "source": [
        "%time\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run2',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.15 µs\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.57s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 951927 tokens\n",
            "Training...\n",
            "[10 | 18.93] loss=3.24 avg=3.24\n",
            "[20 | 31.42] loss=2.98 avg=3.11\n",
            "[30 | 43.91] loss=2.98 avg=3.07\n",
            "[40 | 56.39] loss=2.81 avg=3.00\n",
            "[50 | 68.88] loss=2.76 avg=2.95\n",
            "[60 | 81.35] loss=2.66 avg=2.90\n",
            "[70 | 93.85] loss=2.90 avg=2.90\n",
            "[80 | 106.35] loss=2.66 avg=2.87\n",
            "[90 | 118.83] loss=2.95 avg=2.88\n",
            "[100 | 131.34] loss=2.95 avg=2.89\n",
            "[110 | 143.83] loss=2.84 avg=2.88\n",
            "[120 | 156.31] loss=2.61 avg=2.86\n",
            "[130 | 168.80] loss=2.69 avg=2.84\n",
            "[140 | 181.27] loss=2.87 avg=2.85\n",
            "[150 | 193.74] loss=2.70 avg=2.84\n",
            "[160 | 206.22] loss=2.51 avg=2.81\n",
            "[170 | 218.70] loss=2.66 avg=2.80\n",
            "[180 | 231.19] loss=2.68 avg=2.80\n",
            "[190 | 243.68] loss=2.77 avg=2.80\n",
            "[200 | 256.16] loss=2.72 avg=2.79\n",
            "======== SAMPLE 1 ========\n",
            " I have a very odd feeling about them. \n",
            " There is only one way to go, and that is to take away the one-party-party that's only half the fun. \n",
            " There's only one way for me to live, and that is the other way. \n",
            "' 'I wish I wouldn't,' said Jane as she started to walk past. \n",
            " 'The world's a long way from home, I expect. \n",
            "' 'But it's your fault, ' murmured Mabel eagerly. \n",
            " 'You're not going to make me go away at that; I'm here now to make you go away at other times. \n",
            "' 'There is a reason I don't like to see you here,' said Robert; 'though I hope to see you. \n",
            " It would be better even if I could't go, if everything was in a place that could be saved. \n",
            "' 'I'm sorry, I suppose you won't go. \n",
            "' 'I'd rather get to you,' said Jane again. \n",
            " 'I suppose I would, in the hope that it's somehow out of your power and perhaps out of your time. \n",
            " Do you mind if I'll keep in touch? For goodness' sake, can't we keep in touch in the evening? We all know if all the books are out, that's not going to happen. \n",
            " We could make a letter to all the school-books you like, and they would all read it aloud to themselves; but you wouldn't do that to any one else. \n",
            " Of course every one will, for the more books we have the more likely they are to be out, and we can't possibly lose them all at once. \n",
            " And of course you don't suppose that all the books will be in the same book at all when you keep at it on two different occasions. \n",
            " We must keep an eye out and find those books which are out on two different occasions, and when we're sure that they are, we may make another printing for you. \n",
            " It's too bad we didn't try out the new kind of printing. \n",
            " It is quite a nice printing. \n",
            "' This made Robert feel very lonely. \n",
            " He had been a long time, as he used to say, of being a poet, and of being a writer. \n",
            " But he was very glad that he had come, for he could read poetry with all his mind, without being a poet himself. \n",
            " The new type of printing is very expensive. \n",
            " It costs half its original price, and has been reprocessed and re-rinsed to restore it to the original form, which is very expensive. \n",
            " It costs hundreds of thousands of dollars to do, and can scarcely be sent for by any printing company. \n",
            " The printing is very difficult to manage. \n",
            " For Robert, who has lost two copies of this book over the years, and has been writing poetry instead of poetry for over six years, he has more difficulties in managing than any other poet in the world can. \n",
            " And he still feels very lonely. \n",
            " The new printing seems very nice anyway. \n",
            " And it is an extremely pleasant printing. \n",
            " It was printed from a different kind of paper—a different kind to what we used to use; and it can print in very long, clear sheets without any fancy marks. \n",
            " Everything about this paper is very nice, and very easy to read. \n",
            " It's a bit messy, but that isn't so bad. \n",
            " But it is not that our paper has any fancy marks. \n",
            " It has a very good feeling to the paper that everything has gone smoothly. \n",
            " The more you put and twist it, the happier it gets; and when it is in good shape, every tiny bit of it makes you feel quite good and you will feel a little bit silly. \n",
            " But here the paper is rather heavy, much too thick, and the end of it is only about the right size for Robert. \n",
            " It is not very thick, but it is made of heavy, strong, brownish-brown paper, and it is a very hard thing to twist. \n",
            " You might get one of the big, old, tight tubes that you draw under and pin at the ends, and that will be stiff enough to hold something quite heavy and a bit too thick for Robert. \n",
            " You may, of course, get another very big tube if some one pleases. \n",
            " But if this paper doesn't have any fancy marks it will not print out well or at all, and that is not the sort of paper you want to draw. \n",
            " And even though it is stiff with fine, thick stamps on it on all sides you should not expect to print all over it. \n",
            " Also you will not be able to print a lot of stamps on all the printed things, so you will not be able to tell\n",
            "\n",
            "[210 | 277.54] loss=2.68 avg=2.79\n",
            "[220 | 290.04] loss=2.70 avg=2.78\n",
            "[230 | 302.53] loss=2.43 avg=2.76\n",
            "[240 | 315.02] loss=2.41 avg=2.75\n",
            "[250 | 327.49] loss=2.75 avg=2.75\n",
            "[260 | 339.96] loss=2.61 avg=2.74\n",
            "[270 | 352.42] loss=2.72 avg=2.74\n",
            "[280 | 364.91] loss=2.35 avg=2.73\n",
            "[290 | 377.38] loss=2.40 avg=2.71\n",
            "[300 | 389.85] loss=2.63 avg=2.71\n",
            "[310 | 402.33] loss=2.34 avg=2.70\n",
            "[320 | 414.80] loss=2.44 avg=2.69\n",
            "[330 | 427.27] loss=2.40 avg=2.68\n",
            "[340 | 439.76] loss=2.31 avg=2.66\n",
            "[350 | 452.24] loss=2.42 avg=2.66\n",
            "[360 | 464.72] loss=2.36 avg=2.65\n",
            "[370 | 477.21] loss=2.53 avg=2.64\n",
            "[380 | 489.68] loss=2.44 avg=2.64\n",
            "[390 | 502.15] loss=2.43 avg=2.63\n",
            "[400 | 514.61] loss=2.67 avg=2.63\n",
            "======== SAMPLE 1 ========\n",
            " the       [T]ek                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     .\n",
            "\n",
            "[410 | 534.94] loss=2.42 avg=2.62\n",
            "[420 | 547.41] loss=2.14 avg=2.61\n",
            "[430 | 559.91] loss=2.49 avg=2.61\n",
            "[440 | 572.38] loss=2.26 avg=2.60\n",
            "[450 | 584.86] loss=2.39 avg=2.59\n",
            "[460 | 597.35] loss=2.38 avg=2.59\n",
            "[470 | 609.85] loss=2.54 avg=2.58\n",
            "[480 | 622.33] loss=2.32 avg=2.58\n",
            "[490 | 634.82] loss=2.26 avg=2.57\n",
            "[500 | 647.31] loss=2.32 avg=2.56\n",
            "Saving checkpoint/run2/model-500\n",
            "[510 | 662.78] loss=2.21 avg=2.55\n",
            "[520 | 675.26] loss=2.19 avg=2.55\n",
            "[530 | 687.75] loss=2.36 avg=2.54\n",
            "[540 | 700.24] loss=2.47 avg=2.54\n",
            "[550 | 712.72] loss=2.15 avg=2.53\n",
            "[560 | 725.22] loss=2.17 avg=2.52\n",
            "[570 | 737.70] loss=2.31 avg=2.52\n",
            "[580 | 750.18] loss=2.29 avg=2.51\n",
            "[590 | 762.67] loss=2.20 avg=2.50\n",
            "[600 | 775.15] loss=2.11 avg=2.50\n",
            "======== SAMPLE 1 ========\n",
            "I'll tell you something, just for the sake of telling you, for goodness' sake let's show you the way, and just keep on going down the street. \n",
            "' 'Why do they walk on two legs, and walk on a manger?' 'Well, the manger's the very same as the street,' he answered. \n",
            " 'Well we did, of course we've walked on his own,' explained Anthea. \n",
            " 'The manger's an inch from the manger, and no sooner did we come and walk at him the stone manging was stopped. \n",
            " Then another manger came, and this was the manger that stopped the stone manging. \n",
            " So then the city was stopped and the stone man went away. \n",
            " Then the city was cleared up and the city stood in peace, while the manger that stopped the stone mangy thing went on his way. \n",
            " Anthea and her brother marched on, but when the manger stopped they walked together again and waited for the manger at the gate. \n",
            " At last at last the city was cleared up, but the manger was not there when they got there. \n",
            " He could not get in, and so it was night that the children called the manger out as he put down his iron door. \n",
            " The manger said: 'This city is very ancient, and very dark, the sky only light shines on it when the sun shines. \n",
            " I am bound to keep on, you know. \n",
            "' So the boys built a large house there and slept there until one of the iron legs of the manger came to the roof, with a bright iron trap in his breast. \n",
            " The trap held torches, and the iron leg was lighted with a blue light that the boys did not see. \n",
            " The trap was soon lighted with coal, which the children brought to the house. \n",
            " Then the trap was lighted. \n",
            " The children slept in the house, but it did not last long. \n",
            " The iron trap was lighted with a coal-fire at once, and the children went on reading. \n",
            " It was dark when those iron legs came back, for the manger came back with a stone leg in his hand to light the city. \n",
            " He lighted the city with fire, but to the children the city was very dark, for the stone leg lighted it up, for it would not see the sunlight from the balcony or from the window of the house. \n",
            " When Anthea came to the gate, the one that lighted the city was the one that lighted the house, for they heard the voices saying: 'Here he is, in that house a stone man. \n",
            " To light this city that he comes!' They did not see him again for some time. \n",
            " For they found that it was very early before they were almost at the gate, so they went round into the house to ask the housekeeper to give them the charm, for they would not be allowed out in the evening unless they kept their eyes open, and the charm was handed down from mother to the very first child. \n",
            " In the children's house Anthea had not her iron bed, so the charm had to be carried along with her. \n",
            " To take it home with her Anthea took the Amulet, which is known as the Royal Amulet. \n",
            " At last a dragon awoke, and it was the dragon that lighted the city. \n",
            " A great dragon came from the dark sky, and it was the dragon who lighted the house. \n",
            " Then the houses came back from the dark and no longer the door of that house. \n",
            " The housekeeper said: 'Look here, I've seen some strange people come and light the cities. \n",
            " I will speak to them. \n",
            "' So they went round the house. \n",
            " Outside they said: 'These are the houses that are being lighted. \n",
            "' But they could not hear any of the houses because, being so early in the day, they were invisible, and no one could see them. \n",
            " The city looked brighter, and people came into the house, and lighted it. \n",
            " And no one knew what they were doing. \n",
            " The children went in to see, and no one was near, but in the darkness the darkness was like a dark cave. \n",
            " At last the dragon came to the house, who was as big as the dragon, and dressed the magic armor. \n",
            " He took the Amulet, and he lighted the house with fire, but that did not make the city more lighted, because, as he lighted it, he was not the only one, and he lighted all the others by night. \n",
            " The Amulet came later in the night. \n",
            " To get home in the evening the boys put on a white glove as they got into the house. \n",
            " They were\n",
            "\n",
            "[610 | 795.43] loss=2.22 avg=2.49\n",
            "[620 | 807.90] loss=2.30 avg=2.49\n",
            "[630 | 820.36] loss=2.16 avg=2.48\n",
            "[640 | 832.85] loss=2.38 avg=2.48\n",
            "[650 | 845.33] loss=2.26 avg=2.47\n",
            "[660 | 857.81] loss=2.32 avg=2.47\n",
            "[670 | 870.29] loss=2.26 avg=2.47\n",
            "[680 | 882.79] loss=2.28 avg=2.46\n",
            "[690 | 895.27] loss=2.24 avg=2.46\n",
            "[700 | 907.75] loss=2.31 avg=2.45\n",
            "[710 | 920.25] loss=2.31 avg=2.45\n",
            "[720 | 932.73] loss=2.00 avg=2.44\n",
            "[730 | 945.21] loss=2.40 avg=2.44\n",
            "[740 | 957.68] loss=2.06 avg=2.43\n",
            "[750 | 970.14] loss=2.30 avg=2.43\n",
            "[760 | 982.62] loss=1.99 avg=2.42\n",
            "[770 | 995.06] loss=1.82 avg=2.41\n",
            "[780 | 1007.49] loss=2.12 avg=2.41\n",
            "[790 | 1019.95] loss=2.36 avg=2.41\n",
            "[800 | 1032.41] loss=1.99 avg=2.40\n",
            "======== SAMPLE 1 ========\n",
            " double of his cap, he stopped short and his lips curved into a thin smile. \n",
            " ‘Don’t be frightened,’ said Cyril, ‘this’s nothing to begin with. \n",
            " We might try the Psammead on you, only don’t know whether it would hurt. \n",
            "’ ‘Never mind,’ said the learned gentleman; ‘it was really a pleasant surprise that we found you in the Forest of Masaryk. \n",
            " It was very nice to be in a forest—very nice, indeed—but there wasn’t any spring in it. \n",
            "’ ‘Don’t be frightened, Mabel,’ said Anthea. \n",
            " ‘I shall be happy going anywhere I get my wish. \n",
            " It seems silly being in a sand-pit, but we must try the Psammead. \n",
            " There isn’t any spring in a sand-pit unless there be springs of water. \n",
            "’ ‘I know that, my good child,’ said Robert. \n",
            " The Psammead, which had been sitting on his knee, rubbed its snail’s eyes. \n",
            " ‘I can’t think what you’re going to do with your dearest friend,’ it said; ‘but I’ve been wishing for years that my wish was true and I had really given up, but I’ve never had; besides, I’ve always thought that my wishing would never come true. \n",
            " So now I’ve been wishing ever since I found you! So here’s my dearest man. \n",
            "’ ‘I didn’t have to say exactly how I meant to say I’d let you go—I never did,’ said Anthea. \n",
            " But he just laughed and shook his head. \n",
            " It will not do you any good. \n",
            " You let me come inside your head and, ’ It was in the middle of the hall that the boy’s wife and young children suddenly spoke together. \n",
            " ‘It’s most awful about being burned at the stake! A boy or girl mustn’t have that! If he let you go all night in such a hot and damp place, I couldn’t help but feel that my wishes were sometimes met. \n",
            " And I didn’t know what to do! I couldn’t help how I was feeling, and I couldn’t go! It isn’t fair. \n",
            " Why did you let me come in and sleep under your arm with such a damp place in your chest?’ Then the learned gentleman shook the hand of Robert who stooped to kiss it. \n",
            " It was Robert’s old clergyman. \n",
            " He had it for a living in what was to have been the worst hotel fire in the world. \n",
            " Then the clergyman’s wife and children spoke together before him. \n",
            " One spoke of a nice house in which to stay; the other held a good deal of promise. \n",
            " As a parting thought, she said— ‘Oh, dear! It does feel so good to be together with you!’ ‘And it’s too bad not to get married!’ the learned gentleman answered, ‘I’d rather marry that nice one than a man who let me go; at least, not that one; even if it was a circus—’ The children paused till they had quite forgotten everything. \n",
            " The learned gentleman hesitated a moment. \n",
            " Then he said— ‘Yes. \n",
            "’ ‘It is a most unfortunate accident that I let you go; I wish to be married again some day, and this will certainly be a wonderful marriage-day. \n",
            "’ The clergyman coughed nervously. \n",
            " Anthea could not hear him, but she heard her mother say— “But it’s not fair. \n",
            "” The children saw at once that the clergyman was too old for anything now. \n",
            " They had never heard of a married man and woman in England. \n",
            " The clergyman did not appear to be married. \n",
            " That is not manners—sometimes you need not do anything at all! Even the grown-ups never do anything. \n",
            " The children felt almost as though they had been burnt at the stake. \n",
            " The clergyman was an old gentleman in the trenches. \n",
            " His age and wealth made him a most unsuitable candidate. \n",
            " The old gentleman had not a navy greengrocer’s beard, which an Irishman should be proud of, and a clean cut, which an Englishman would be proud of. \n",
            " He bore an uncle who was as proud of him as an Italian\n",
            "\n",
            "[810 | 1052.69] loss=1.94 avg=2.39\n",
            "[820 | 1065.18] loss=2.01 avg=2.38\n",
            "[830 | 1077.66] loss=1.99 avg=2.38\n",
            "[840 | 1090.13] loss=1.90 avg=2.37\n",
            "[850 | 1102.60] loss=1.86 avg=2.36\n",
            "[860 | 1115.09] loss=2.04 avg=2.35\n",
            "[870 | 1127.57] loss=1.59 avg=2.34\n",
            "[880 | 1140.06] loss=1.97 avg=2.33\n",
            "[890 | 1152.53] loss=1.96 avg=2.33\n",
            "[900 | 1165.00] loss=2.11 avg=2.32\n",
            "[910 | 1177.45] loss=2.03 avg=2.32\n",
            "[920 | 1189.93] loss=2.06 avg=2.32\n",
            "[930 | 1202.40] loss=2.19 avg=2.31\n",
            "[940 | 1214.88] loss=1.92 avg=2.31\n",
            "[950 | 1227.36] loss=1.76 avg=2.30\n",
            "[960 | 1239.84] loss=1.97 avg=2.29\n",
            "[970 | 1252.31] loss=2.06 avg=2.29\n",
            "[980 | 1264.80] loss=2.08 avg=2.29\n",
            "[990 | 1277.28] loss=2.02 avg=2.28\n",
            "[1000 | 1289.76] loss=1.83 avg=2.27\n",
            "Saving checkpoint/run2/model-1000\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K"
      },
      "source": [
        "After training, we copy the checkpoint folder to our Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3"
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run2')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQv9JFwa79td"
      },
      "source": [
        "## 3. Generating Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "### 3.1. Load the Model Checkpoint\n",
        "\n",
        "The next cell copies the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "# Copy checkpoint file into Colab\n",
        "gpt2.copy_checkpoint_from_gdrive(run_name='run2')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV"
      },
      "source": [
        "From Colab, we load the retrained model checkpoint and metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** To re-run this cell, it is important to **restart the VM first** incl. rerunning libraries/installs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef058c69-73b2-4520-8df4-7c21b6816f0f"
      },
      "source": [
        "# Loading the re-trained model checkpoint and metadata\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run2')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/run2/model-1000\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run2/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "### 3.2. Generate Text From The Trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih7JL-7r-HjX"
      },
      "source": [
        "We use `generate` to generate text from the loaded model. Optional parameters for `gpt2.generate` include:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f6d77b-7a79-41da-ed15-3cd631b779b5"
      },
      "source": [
        "gpt2.generate(sess, run_name='run2')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To understand the Games well, you must first understand what the good people of that country are like. \n",
            " They have very generous and kind-hearted parents, and a very strong and determined and prosperous guild. \n",
            " They are very industrious, and they are all highly respected. \n",
            " The children of these people are the most valuable and the bravest. \n",
            " They have the most splendid hopes—though they must always be hopeful at the worst possible moments. \n",
            " They are most industrious in their preparations, and their hearts are most fervent in the pursuit of their hopes. \n",
            " They are said to be the guardians of mankind—a most unvarying and reliable regard. \n",
            " They are very good-hearted and patient. \n",
            " They have the most splendid minds, and their minds are always ready to work for the good of mankind. \n",
            " And now it is time we began to understand what they are like. \n",
            "                                                                                                                                Below is a list of the various classes of the people, and their various capacities, of course. \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfdd101-deec-47a8-bea0-0dca5baaffbd"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              prefix=\"Once upon a time\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Once upon a time there was a king, and his kings were fair and upright. \n",
            " The children could not tell whether he was a king-builder or a king-builder's son. \n",
            " But the king-builder knew a king, and he made him a king-builder's son. \n",
            " 'Then all of a sudden,' said the child, 'there was a king, and he took away all the children of the land. \n",
            " Then he placed in a cave some ice, and he\n",
            "====================\n",
            "Once upon a time vast numbers of people were able to live in the great cities of the world, and millions of them were citizens of those great nations. \n",
            " And these people were rude, ignorant, stupid, uncaring, stupidly happy, uncaring, uncaring, stupidly wicked. \n",
            " The cities were like dream-changes in history. \n",
            "       'I was born in the great city and I built this city,' said the King of Babylon, 'to keep the\n",
            "====================\n",
            "Once upon a time the gods and goddesses of that city and its people had their share in the flourishing of the country. \n",
            " For through their presence all became aperçus, and the local people became a people. \n",
            " And the sons and daughters of the gods, who dwelt there, were able to reap the harvest and to reap the harvest of life, and were crowned with the title of King-Beasts. \n",
            " And when the gods and goddesses were assembled in the city, the\n",
            "====================\n",
            "Once upon a time, the Egyptian king ruled over the world. \n",
            " And, of course, he had enough to do with magic to keep people happy and to keep them going for ever. \n",
            " It was not, as the saying was, his own doing. \n",
            " But people did, and he had to do something to bring people about. \n",
            " The charm was made by the children in the shadow of an old hall. \n",
            " It was the most wonderful charm ever, and they called it the Hall\n",
            "====================\n",
            "Once upon a time that the gods and goddesses from all over the world had come down to meet them at their dwelling. \n",
            " And the gods were not in the least alarmed at being welcomed by any one of them. \n",
            " For, as the saying goes, 'there is no fear all day long!' And the gods, being strangers to the island, were surprised to meet strangers from all over the world. \n",
            " And the gods were astonished to find an inhabitant of a known world, and to see\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzouL-yTHt_p"
      },
      "source": [
        "seed = \"Our story begins with an king that lived in his castle with his queen and their two children and they ruled over a large kingdom of happy people.\"\n",
        "max_len = 350"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJRhvxSmHpuy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b70907-43a9-4808-9b36-31bd5917a2bb"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=seed,\n",
        "              nsamples=5,\n",
        "              batch_size=5,\n",
        "              top_k = 40\n",
        "              )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Our story begins with an king that lived in his castle with his queen and their two children and they ruled over a large kingdom of happy people. \n",
            " Then the queen sent men to take the children as captives and to make them strong enough to become strong enough to fight, and she sent men of that tribe to dig for treasure and to carry the captives up the tower. \n",
            " And then the king took the children with him to seek refuge in the mountains. \n",
            " And the children were strong men, and the king took them with him to seek the land of the nymphs, the nymphs of the wood where the charm is, and he built a great city there. \n",
            "' 'And now,' said Jane, 'the children are strong men, and the king took them with him to seek the land of the blue-fish, the blue-fish of the sea, and he built a great city there. \n",
            "' 'And so we are told,' said Anthea, 'and so are all your ancestors, you and your baby brother and your baby sister and your baby brother's uncle, and your and your baby brother's grandmother. \n",
            "' 'And of course,' said the Lamb, 'you're not mixed up in mixed-up things, and that's all. \n",
            "' 'All mixed up is the nymphs,' said\n",
            "====================\n",
            "Our story begins with an king that lived in his castle with his queen and their two children and they ruled over a large kingdom of happy people. \n",
            " The son of a great man, the king, with his wife were three sons, and they were called the Knooks. \n",
            "  The king was a great man, but his people were weak and infidels, and he hated the living. \n",
            " The Knooks ruled over the king's subjects and the wicked and infidels ruled over the wicked and infidels ruled over the living. \n",
            " The King wanted to take the children to the High King, and he said to the children,  ‘You can be a king for living and a king for evil, but you must not be a defilement to the lives of the children. \n",
            "  You must not be a crime to the children and to the king. \n",
            "  Then the children would not be evil and would not be cruel, and the king would not harm them. \n",
            "  For there is no evil in what you do. \n",
            "  Therefore you should not harm the children. \n",
            "  And you would not take children from the children. \n",
            "  Because there is no evil in what you do. \n",
            "  Therefore you should not harm the children. \n",
            "  And you should not take the children from\n",
            "====================\n",
            "Our story begins with an king that lived in his castle with his queen and their two children and they ruled over a large kingdom of happy people. \n",
            "  At first they thought it was a joke, but the more they thought the more they wanted something real. \n",
            "  When the king's people were ready to go to war the people came out and came to live in the little, brown house. \n",
            "  There were no horses, only buns and candied fruit. \n",
            "  It was a very small, beautiful, nice house, with a fountain and a green tree, and the king's people came out and saw the buns and candied fruit. \n",
            " 'How do you do?' said the king's people. \n",
            "  'We come out of the wood to have fun, and we get there first, with our swords and our daggers and our plaice. \n",
            "' 'Then we have to fight, and then we have to give one blow or the other. \n",
            "' 'If you'll be happy with that, then you're our equal,' said the king's people. \n",
            "  'And you've done it by killing the king and robbing his house. \n",
            "  That's done, and it's the end of our fight. \n",
            "  But you're our enemies, so be careful!' And with\n",
            "====================\n",
            "Our story begins with an king that lived in his castle with his queen and their two children and they ruled over a large kingdom of happy people. \n",
            "  They were not of the kind of people that are not called the good people, but they were good and kind, and gave their own way to living. \n",
            "  They were well-armed with horses, and had a goodly army ready for battle. \n",
            " But as the years passed they became less able to hold on to their rule, and only suffered from loneliness and the need to communicate with their neighbors. \n",
            "  The king's wife was the daughter of the king's own son, and he had a goodly army, and sent envoys to the king's court to say how sorry he was that he was not imprisoned. \n",
            "  The king's wife was the daughter of the king's own son, and she was very noble and kind. \n",
            "  The king's wife was the husband of the king's son, and he had a goodly army, and sent envoys to the king's court to say how glad he was that he was not imprisoned. \n",
            "  The king's wife was the son of the king's own son, and he had a goodly army, and sent envoys to the king's court to say how proud he was that he had made such a king\n",
            "====================\n",
            "Our story begins with an king that lived in his castle with his queen and their two children and they ruled over a large kingdom of happy people. \n",
            " They were able to become happy because they were able to have children of any age. \n",
            " But the evil King could not keep children, so he made them to have no children at all. \n",
            " His evil conduct destroyed the good King and he cut off the children from the children of the happy people. \n",
            " Then the evil King suddenly called and said,  ‘I am the destroyer of children’ and he cut off all children’ hands. \n",
            " Then the children’s hands were filled with dust, so he said,  ‘These children are  not children,’ and then he said,  ‘All are created of the same immortal substance;  they’ll be happy and full in the future;  they’ll be strong and happy and immortal, and be immortal to the nought. \n",
            "’ And this he did, for he had seen the Amulet and knew that it was true. \n",
            " The children were so full of gladness and gladness that they felt as though they had been made happy before, for the Amulet was now held in their hands. \n",
            " They had never before seen it. \n",
            " The Amulet grew\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}