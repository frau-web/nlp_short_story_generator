{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_w08XxKB_xI"
      },
      "source": [
        "First we import the relevant files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJTVtfBXZ7Sx",
        "outputId": "5aed2594-2f69-4e77-a7d9-5c0dcf110b86"
      },
      "source": [
        "import nltk, re, pprint\n",
        "from nltk import word_tokenize\n",
        "import os\n",
        "import codecs\n",
        "os.listdir('.')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', '.ipynb_checkpoints', 'FinalLarge.txt', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOMFFmH7CEbB"
      },
      "source": [
        "Google collab is missing many parts of nltk, so I download them manually here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnkqZTGFdU0E",
        "outputId": "35263e69-da0b-479f-d724-48aac6258091"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('treebank')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVhzBv4L0BbM"
      },
      "source": [
        "# Import NLTK module\n",
        "import nltk\n",
        "\n",
        "# Import word_tokenize \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Import POS tagger\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "f = open(\"FinalLarge.txt\", 'r')\n",
        "raw = f.read()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA9UCFA-2zaO"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    This function takes a text. Split it in tokens using word_tokenize. \n",
        "    And then tags them using pos_tag from NLTK module.\n",
        "    It outputs a list of tuples. Each tuple consists of a word and the tag with its \n",
        "    part of speech.\n",
        "    \"\"\"\n",
        "    # Get the tokens\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Tags the tokens\n",
        "    tagging = nltk.pos_tag(tokens)\n",
        "    # Returns the list of tuples\n",
        "    return tagging"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YylF3iv44IVF"
      },
      "source": [
        "label_text = preprocess_text(raw)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg-WxzXL4TSu",
        "outputId": "6187859e-08fd-4b64-ff59-c380ae35cc91"
      },
      "source": [
        "for i in range(0, 20, 5):\n",
        "    print(label_text[i], label_text[i+1], label_text[i+2], label_text[i+3], \n",
        "          label_text[i+4])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('text', 'IN') ('A', 'NNP') ('duck', 'NN') ('made', 'VBD') ('her', 'PRP$')\n",
            "('nest', 'JJS') ('under', 'IN') ('some', 'DT') ('leaves', 'NNS') ('.', '.')\n",
            "('She', 'PRP') ('sat', 'VBD') ('on', 'IN') ('the', 'DT') ('eggs', 'NNS')\n",
            "('to', 'TO') ('keep', 'VB') ('them', 'PRP') ('warm', 'JJ') ('.', '.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ljcRM2g51fA"
      },
      "source": [
        "# Define the rule\n",
        "rule = \"NP: {<DT>?<JJ>*<NNP>+}\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8r05aTk6D34"
      },
      "source": [
        "# We define the parser using the rule\n",
        "parser = nltk.RegexpParser(rule) \n",
        "\n",
        "# Apply to the tagged words\n",
        "result = parser.parse(label_text) \n",
        "\n",
        "named_entities = nltk.ne_chunk(label_text)\n",
        "\n",
        "# Print only the chunks\n",
        "chunklist = []\n",
        "for entity in named_entities:\n",
        "    if type(entity) == nltk.tree.Tree:\n",
        "        chunklist.append(str(entity))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIGKV4owW_fy",
        "outputId": "78a52087-906f-4737-92ee-bcc4483df421"
      },
      "source": [
        "chunklist[0:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(PERSON Splash/NN)',\n",
              " '(PERSON Splash/NN)',\n",
              " '(ORGANIZATION LITTLE/JJ)',\n",
              " '(ORGANIZATION PINE/NNP)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84VQ8cdnln79"
      },
      "source": [
        "elements_count = {}\n",
        "# iterating over the elements for frequency\n",
        "for element in chunklist:\n",
        "   # checking whether it is in the dict or not\n",
        "   if element in elements_count:\n",
        "      # incerementing the count by 1\n",
        "      elements_count[element] += 1\n",
        "   else:\n",
        "      # setting the count to 1\n",
        "      elements_count[element] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90e42wCerWvD"
      },
      "source": [
        "Character_count={}\n",
        "# printing the elements frequencies\n",
        "for key, value in elements_count.items():\n",
        "   Character_count[key] = value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edJcEWy86FkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2a8044-9879-49fc-efa6-095919e1847a"
      },
      "source": [
        "sorted_values = sorted(Character_count.values()) # Sort the values\n",
        "sorted_dict = {}\n",
        "\n",
        "for i in sorted_values:\n",
        "    for k in Character_count.keys():\n",
        "        if Character_count[k] == i:\n",
        "            sorted_dict[k] = Character_count[k]\n",
        "            break\n",
        "\n",
        "print(sorted_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'(ORGANIZATION PINE/NNP)': 1, '(PERSON Splash/NN)': 2, '(ORGANIZATION LITTLE/JJ)': 3, '(PERSON Fire/NNP)': 4, '(PERSON Asgard/NNP)': 5, '(PERSON Mittens/NNP)': 6, '(PERSON Thrym/NNP)': 7, '(PERSON Poor/NNP)': 8, '(PERSON Gretchen/NNP)': 9, '(PERSON Sif/NNP)': 10, '(PERSON Mamma/NNP)': 11, '(PERSON Medea/NNP)': 12, '(PERSON God/NNP)': 13, '(PERSON Tabitha/NNP)': 14, '(ORGANIZATION Doctor/NNP)': 15, '(ORGANIZATION Wolf/NNP)': 16, '(PERSON Moppet/NNP)': 17, '(ORGANIZATION Vicar/NNP)': 18, '(PERSON Bessie/NNP)': 19, '(ORGANIZATION Christmas/NNP)': 20, '(PERSON Brok/NNP)': 21, '(PERSON Thor/NNP)': 22, '(PERSON Aunt/NNP Emma/NNP)': 23, '(PERSON Look/NNP)': 24, '(GPE Mother/NNP)': 25, '(GPE Egypt/NNP)': 26, '(PERSON Lucie/NNP)': 27, '(PERSON Tom/NNP Kitten/NNP)': 28, '(GPE English/NNP)': 29, '(GPE Captain/NNP)': 30, '(PERSON Nurse/NNP)': 31, '(ORGANIZATION Laughing/NNP Valley/NNP)': 32, '(GPE England/NNP)': 33, '(PERSON Johnson/NNP)': 34, '(PERSON Cathy/NNP)': 35, '(PERSON Necile/NNP)': 36, '(ORGANIZATION Amulet/NNP)': 37, '(ORGANIZATION Forest/NNP)': 38, '(PERSON Phil/NNP)': 39, '(GPE Mabel/NNP)': 40, '(ORGANIZATION Knooks/NNP)': 41, '(PERSON Lord/NNP Yalding/NNP)': 44, '(PERSON Jason/NNP)': 45, '(PERSON Jim/NNP)': 47, '(PERSON Caesar/NNP)': 48, '(GPE Mademoiselle/NNP)': 49, '(PERSON Martin/NNP)': 54, '(GPE Indian/JJ)': 55, '(PERSON Loki/NNP)': 57, '(ORGANIZATION THE/NNP)': 59, '(PERSON Mademoiselle/NNP)': 60, '(PERSON Roberta/NNP)': 61, '(GPE London/NNP)': 64, '(ORGANIZATION Curlytops/NNP)': 69, '(PERSON Percy/NNP)': 72, '(GPE French/JJ)': 73, '(PERSON Don/NNP)': 77, '(PERSON Garnet/NNP)': 81, '(PERSON Nicknack/NNP)': 89, '(PERSON Perks/NNP)': 94, '(PERSON Martha/NNP)': 98, '(PERSON Aunt/NNP Harriet/NNP)': 104, '(ORGANIZATION Princess/NNP)': 115, '(PERSON Alice/NNP)': 122, '(PERSON Dora/NNP)': 133, '(PERSON Hal/NNP)': 140, '(PERSON Janet/NNP)': 142, '(PERSON Dicky/NNP)': 144, '(PERSON Eliza/NNP)': 149, '(PERSON Trouble/NNP)': 150, '(GPE Gerald/NNP)': 158, '(ORGANIZATION Lamb/NNP)': 166, '(PERSON Mr./NNP Noah/NNP)': 185, '(GPE Winona/NNP)': 189, '(PERSON Father/NNP)': 217, '(PERSON Oswald/NNP)': 222, '(PERSON Ted/NNP)': 241, '(PERSON Jan/NNP)': 244, '(PERSON Claus/NNP)': 273, '(ORGANIZATION Psammead/NNP)': 276, '(PERSON Kathleen/NNP)': 284, '(PERSON Phyllis/NNP)': 287, '(ORGANIZATION Phoenix/NNP)': 305, '(PERSON Winona/NNP)': 320, '(PERSON Lucy/NNP)': 330, '(PERSON Jimmy/NNP)': 333, '(PERSON Mabel/NNP)': 385, '(PERSON Mother/NNP)': 422, '(PERSON Gerald/NNP)': 465, '(PERSON Bobbie/NNP)': 474, '(PERSON Uncle/NNP Wiggily/NNP)': 517, '(PERSON Philip/NNP)': 525, '(PERSON Peter/NNP)': 562, '(PERSON Jane/NNP)': 615, '(PERSON Cyril/NNP)': 766, '(PERSON Robert/NNP)': 848, '(PERSON Anthea/NNP)': 917}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSDQJO3YhQLQ"
      },
      "source": [
        "tokens = nltk.tokenize.word_tokenize(raw)\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "verbs = [token[0] for token in tagged_tokens if token[1] in [\"VB\",\"VBG\",'VBD',\"VBN\",'VBP',\"VBZ\"]]\n",
        "frequency = nltk.FreqDist(verbs)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D70JxlWLgV6"
      },
      "source": [
        "For Actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os7BeP4zLoPx",
        "outputId": "7326514d-4ef9-4018-e9db-7fde27af841a"
      },
      "source": [
        "sorted_values = sorted(dict(frequency).values()) # Sort the values\n",
        "sorted_dict = {}\n",
        "freq_dict = dict(frequency)\n",
        "for i in sorted_values:\n",
        "    for k in freq_dict.keys():\n",
        "        if freq_dict[k] == i:\n",
        "            sorted_dict[k] = freq_dict[k]\n",
        "            break\n",
        "\n",
        "print(sorted_dict)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'best.': 1, 'pretty': 2, 'Again': 3, 'growl': 4, \"'When\": 5, 'duckling': 6, 'did.': 7, 'can.': 8, 'cooking': 9, 'cooked': 10, 'warm': 11, 'sail': 12, \"'The\": 13, 'spoiled': 14, 'tumbled': 15, 'sailed': 16, 'dying': 17, 'dress': 18, 'cook': 19, 'tore': 20, 'leaves': 21, 'glad': 22, 'herself': 23, 'burning': 24, 'fetched': 25, 'eaten': 26, 'bit': 27, 'singing': 28, 'lifted': 29, 'lose': 30, 'buried': 31, 'swim': 32, 'blew': 33, 'gathered': 34, 'interrupted': 35, 'killed': 36, 'flying': 37, 'dear': 38, 'kill': 39, 'eating': 40, 'stayed': 41, 'calling': 42, 'brave': 43, 'locked': 44, \"'Let\": 45, 'shining': 46, 'hit': 47, 'changed': 48, 'kissed': 49, 'died': 50, 'break': 51, 'stuck': 52, 'fall': 53, 'woke': 54, 'climbed': 55, 'forget': 56, 'broke': 57, 'broken': 58, 'ate': 59, 'lighted': 60, 'shook': 61, 'putting': 62, 'dressed': 65, 'grow': 66, 'growing': 67, 'care': 68, 'filled': 69, 'talked': 70, 'helped': 71, 'pay': 72, 'running': 73, 'dropped': 76, 'wrote': 77, 'drew': 78, 'playing': 79, 'picked': 80, \"'Do\": 81, 'returned': 82, 'met': 83, 'flew': 84, 'frightened': 85, 'stand': 86, 'meet': 87, 'goes': 88, 'faces': 89, 'exclaimed': 90, 'catch': 92, 're': 93, 'laid': 94, 'sit': 95, 'write': 96, 'mind': 97, 'standing': 99, 'cut': 100, 'jumped': 101, 'guess': 102, 'till': 103, 'carried': 105, 'liked': 107, 'bring': 109, 'lived': 110, 'sitting': 111, 'agreed': 112, 'talking': 114, 'expect': 119, 'sleep': 120, 'comes': 122, 'talk': 123, 'stop': 124, 'live': 125, 'won': 126, 'taking': 128, 'trying': 130, 'followed': 131, 'showed': 132, 'grew': 133, 'Let': 134, 'lost': 135, 'given': 136, 'saying': 137, 'walked': 138, 'sent': 139, 'kept': 141, 'laughed': 144, 'seem': 147, 'making': 150, 'round': 151, 'doing': 152, 'caught': 153, 'started': 156, 'Do': 158, 'stay': 162, 'taken': 167, 'stopped': 168, 'hurt': 170, 'lay': 171, 'run': 172, 'says': 173, 'play': 175, 'spoke': 176, 'hear': 177, 'remember': 178, 'opened': 179, 'set': 180, 'suppose': 188, 'fell': 190, 'brought': 192, 'try': 196, 'tried': 197, 'answered': 205, 'feel': 207, 'used': 210, 'eat': 219, 'ran': 222, 'added': 224, 'help': 228, 'coming': 229, 'ask': 238, 'held': 240, 'happened': 251, 'sat': 252, 'wanted': 259, 'keep': 262, 'believe': 271, 'gave': 272, 'mean': 276, 'gone': 285, 'looking': 287, 'left': 293, 'does': 302, 'turned': 307, 'like': 308, 'called': 312, 'wish': 331, 'stood': 334, 'heard': 339, 'done': 364, 'began': 370, 'felt': 380, 'don': 383, 'am': 396, 'give': 397, 'look': 406, 'knew': 423, 'seemed': 444, 'took': 445, 'found': 459, 'told': 476, 'find': 477, 'cried': 525, 'let': 527, 'saw': 536, \"'ve\": 564, \"'re\": 607, 'take': 631, 'want': 632, 'put': 675, 'thought': 676, 'looked': 686, \"'m\": 689, '‘': 698, 'tell': 701, 'make': 707, 'going': 756, 'made': 789, 's': 813, 'say': 836, 'come': 901, '”': 914, 'asked': 923, 'came': 964, 'think': 988, 'got': 1068, '“': 1127, 'get': 1128, 'went': 1214, 'been': 1297, 'see': 1345, 'know': 1411, 'go': 1428, 'are': 1606, 'did': 1658, \"'s\": 2134, '’': 2474, 'do': 2609, 'have': 2679, 'were': 2893, 'is': 2912, 'be': 3286, 'had': 4929, 'said': 7990, 'was': 8737}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLOIxxclLeDY"
      },
      "source": [
        "For Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNOpbU3gGGiH"
      },
      "source": [
        "from collections import Counter\n",
        "z = tokens\n",
        "relative_time_counter = {}\n",
        "relative_time_list = [\"today\",\"yesterday\",\"tomorrow\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\", \"Saturday\",\"Sunday\",\"January\",\"February\",\"March\",\"April\",\"May\", \"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
        "for n in relative_time_list:\n",
        "  relative_time_counter[n] = Counter(z)[n]\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpYlVO1-K3qw",
        "outputId": "d768f8da-4b92-4cc7-be8c-d2f376b660ec"
      },
      "source": [
        "sorted_values = sorted(relative_time_counter.values()) # Sort the values\n",
        "sorted_dict = {}\n",
        "\n",
        "for i in sorted_values:\n",
        "    for k in relative_time_counter.keys():\n",
        "        if relative_time_counter[k] == i:\n",
        "            sorted_dict[k] = relative_time_counter[k]\n",
        "            break\n",
        "\n",
        "print(sorted_dict)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'February': 0, 'April': 1, 'January': 5, 'Friday': 6, 'Tuesday': 7, 'Monday': 8, 'Thursday': 9, 'September': 15, 'November': 16, 'today': 22, 'tomorrow': 23, 'May': 26, 'Sunday': 38, 'yesterday': 40, 'Saturday': 45}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNoL4Nwlf0-_"
      },
      "source": [
        "# Most Common:\n",
        "\n",
        "Names:\n",
        "1. Anthea\n",
        "2. Robert\n",
        "3. Cyril\n",
        "4. Jane\n",
        "5. Peter\n",
        "\n",
        "Places:\n",
        "1. London\n",
        "2. Forest\n",
        "3. England\n",
        "4. Laughing Valley\n",
        "5. Egypt\n",
        "\n",
        "Time: \n",
        "1. Saturday\n",
        "2. yesterday\n",
        "3. Sunday\n",
        "4. May\n",
        "5. tomorrow\n",
        "\n",
        "Actions:\n",
        "1. was\n",
        "2. said\n",
        "3. had\n",
        "4. be\n",
        "5. is\n",
        "6. were\n",
        "7. do\n",
        "8. did\n",
        "9. are\n",
        "10. go\n",
        "11. know\n",
        "12. see\n",
        "13. been\n",
        "14. went\n",
        "15. get\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj4W3Fvafx93"
      },
      "source": [
        "Trying out Spacy to do the same thing - NOT USED DUE TO NOT ENOUGH RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDzqzYDMtwFE"
      },
      "source": [
        "#!pip install -U spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.max_length = 3500000\n",
        "\n",
        "# Process whole documents\n",
        "file_name = \"FinalLarge.txt\"\n",
        "text = open(file_name).read()\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "            chunk.root.head.text)\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}